# TensorRT Backend for Music Source Separation

Bu proje artÄ±k **NVIDIA TensorRT** backend desteÄŸi ile gelir. TensorRT, PyTorch modellerini optimize ederek inference hÄ±zÄ±nÄ± Ã¶nemli Ã¶lÃ§Ã¼de artÄ±rÄ±r.

## ğŸš€ Ã–zellikler

- âœ… TensorRT ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ inference
- âœ… FP16 ve FP32 precision desteÄŸi
- âœ… Otomatik TensorRT engine Ã¶nbellekleme
- âœ… .ckpt checkpoint dosyalarÄ± ile uyumlu
- âœ… PyTorch ile karÅŸÄ±laÅŸtÄ±rmalÄ± benchmark araÃ§larÄ±
- âœ… Geriye dÃ¶nÃ¼k uyumluluk (TensorRT yoksa normal PyTorch kullanÄ±lÄ±r)

## ğŸ“‹ Gereksinimler

### CUDA ve TensorRT Kurulumu

1. **NVIDIA GPU Driver** (Latest)
2. **CUDA Toolkit** (11.x veya 12.x)
3. **TensorRT**

### Python Paketleri

```bash
# TensorRT iÃ§in gerekli paketler
pip install torch2trt
pip install nvidia-tensorrt

# Veya tÃ¼m gereksinimleri yÃ¼kle
pip install -r requirements.txt
```

## ğŸ¯ KullanÄ±m

### 1. TensorRT ile Inference

```bash
python inference_tensorrt.py \
    --model_type mel_band_roformer \
    --config_path ckpts/config.yaml \
    --start_check_point ckpts/model.ckpt \
    --input_folder input/ \
    --store_dir output/ \
    --tensorrt_precision fp16 \
    --use_tensorrt_cache
```

### 2. Parametreler

- `--tensorrt_precision`: TensorRT hassasiyeti (`fp32` veya `fp16`)
- `--use_tensorrt_cache`: TensorRT engine'i Ã¶nbelleÄŸe al (hÄ±z kazancÄ±)
- `--tensorrt_cache_dir`: Ã–nbellek dizini (varsayÄ±lan: `trt_cache/`)

### 3. Normal Inference (PyTorch)

TensorRT yÃ¼klÃ¼ deÄŸilse, otomatik olarak normal PyTorch inference kullanÄ±lÄ±r:

```bash
python inference.py \
    --model_type mel_band_roformer \
    --config_path ckpts/config.yaml \
    --start_check_point ckpts/model.ckpt \
    --input_folder input/ \
    --store_dir output/
```

## ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

### Benchmark Ã‡alÄ±ÅŸtÄ±rma

```bash
python benchmark_tensorrt.py \
    --model_type mel_band_roformer \
    --config_path ckpts/config.yaml \
    --start_check_point ckpts/model.ckpt \
    --device cuda:0 \
    --precision fp16 \
    --num_iterations 100
```

### Beklenen Performans KazanÃ§larÄ±

| Model | PyTorch (ms) | TensorRT FP16 (ms) | HÄ±zlanma |
|-------|--------------|-------------------|----------|
| MelBand-Roformer | ~150 | ~50 | **3.0x** |
| BS-Roformer | ~180 | ~60 | **3.0x** |
| MDX23C | ~100 | ~40 | **2.5x** |

*Not: Performans GPU modeline gÃ¶re deÄŸiÅŸir. YukarÄ±daki deÄŸerler RTX 3090 iÃ§in Ã¶rnek deÄŸerlerdir.*

## ğŸ”§ TensorRT Backend API

### Programatik KullanÄ±m

```python
from tensorrt_backend import TensorRTBackend, load_model_from_checkpoint
from utils import get_model_from_config

# Model yÃ¼kle
model, config = get_model_from_config('mel_band_roformer', 'config.yaml')
model = load_model_from_checkpoint('model.ckpt', model, 'cuda:0')

# TensorRT backend oluÅŸtur
trt_backend = TensorRTBackend(device='cuda:0', cache_dir='trt_cache')

# Model'i compile et
trt_backend.compile_model(
    model=model,
    input_shape=(1, 2, 352800),  # (batch, channels, chunk_size)
    precision='fp16',
    use_cache=True
)

# Inference
import torch
dummy_input = torch.randn(1, 2, 352800).cuda()
output = trt_backend(dummy_input)
```

### TensorRT Engine Kaydetme/YÃ¼kleme

```python
# Engine'i kaydet
trt_backend.save_engine('model.trt')

# Engine'i yÃ¼kle
trt_backend.load_engine('model.trt')
```

## ğŸ“ Proje YapÄ±sÄ±

```
.
â”œâ”€â”€ tensorrt_backend.py          # TensorRT backend implementasyonu
â”œâ”€â”€ inference_tensorrt.py        # TensorRT ile inference scripti
â”œâ”€â”€ benchmark_tensorrt.py        # Benchmark aracÄ±
â”œâ”€â”€ README_TensorRT.md          # Bu dosya
â”œâ”€â”€ trt_cache/                  # TensorRT engine Ã¶nbelleÄŸi
â”‚   â””â”€â”€ *.trt                   # DerlenmiÅŸ TensorRT engine'leri
â””â”€â”€ requirements.txt            # GÃ¼ncellenmiÅŸ gereksinimler
```

## âš™ï¸ TensorRT Ã–nbellekleme

TensorRT engine'leri ilk derleme sÄ±rasÄ±nda Ã¶nbelleÄŸe alÄ±nÄ±r:

```
trt_cache/
â””â”€â”€ abc123def456.trt  # Model + config + precision hash'i
```

### Ã–nbelleÄŸi Temizleme

```bash
rm -rf trt_cache/
```

### Ã–nbellek DavranÄ±ÅŸÄ±

- Ä°lk Ã§alÄ±ÅŸtÄ±rma: Model compile edilir (~1-5 dakika)
- Sonraki Ã§alÄ±ÅŸtÄ±rmalar: Ã–nbellekten yÃ¼klenir (~1 saniye)
- Model/config deÄŸiÅŸirse: Otomatik yeniden compile edilir

## ğŸ› Troubleshooting

### TensorRT BulunamadÄ±

```
ImportError: No module named 'tensorrt'
```

**Ã‡Ã¶zÃ¼m:**
```bash
pip install nvidia-tensorrt
pip install torch2trt
```

### CUDA Out of Memory

```
RuntimeError: CUDA out of memory
```

**Ã‡Ã¶zÃ¼m:**
- Batch size'Ä± azaltÄ±n: `--batch_size 1`
- Chunk size'Ä± azaltÄ±n: `--chunk_size 100000`
- FP32 yerine FP16 kullanÄ±n: `--tensorrt_precision fp16`

### TensorRT Compilation Failed

TensorRT compile edilemezse, otomatik olarak PyTorch'a geri dÃ¶ner:

```
âœ— TensorRT compilation failed: ...
Falling back to PyTorch model
```

### YavaÅŸ Ä°lk Ã‡alÄ±ÅŸtÄ±rma

Ä°lk Ã§alÄ±ÅŸtÄ±rma TensorRT compilation nedeniyle yavaÅŸ olabilir (1-5 dakika). Sonraki Ã§alÄ±ÅŸtÄ±rmalar Ã¶nbellekten yÃ¼klendiÄŸi iÃ§in hÄ±zlÄ±dÄ±r.

## ğŸ” TensorRT vs PyTorch KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik | PyTorch | TensorRT |
|---------|---------|----------|
| Kurulum | Kolay | Orta |
| Ä°lk Ã§alÄ±ÅŸtÄ±rma | HÄ±zlÄ± | YavaÅŸ (compile) |
| Inference hÄ±zÄ± | Normal | **Ã‡ok hÄ±zlÄ±** |
| Memory kullanÄ±mÄ± | Normal | Daha az |
| GPU desteÄŸi | TÃ¼m GPU'lar | NVIDIA GPU gerekli |
| Precision | FP32 | FP16/FP32/INT8 |

## ğŸ“š Ek Kaynaklar

- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [torch2trt GitHub](https://github.com/NVIDIA-AI-IOT/torch2trt)
- [PyTorch Documentation](https://pytorch.org/docs/)

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Ä°lk KullanÄ±mda:** `--use_tensorrt_cache` kullanÄ±n, sonraki Ã§alÄ±ÅŸtÄ±rmalar Ã§ok daha hÄ±zlÄ± olur
2. **FP16 KullanÄ±n:** Modern GPU'larda (RTX 20xx+) FP16 hem hÄ±zlÄ± hem doÄŸru
3. **Batch Size:** MÃ¼mkÃ¼nse batch size artÄ±rÄ±n (GPU memory izin veriyorsa)
4. **Warmup:** Ä°lk birkaÃ§ inference yavaÅŸ olabilir, sistem Ä±sÄ±ndÄ±kÃ§a hÄ±zlanÄ±r

## ğŸ“ Destek

Sorun yaÅŸarsanÄ±z:
1. `benchmark_tensorrt.py` ile test edin
2. PyTorch inference ile karÅŸÄ±laÅŸtÄ±rÄ±n
3. TensorRT loglarÄ±nÄ± kontrol edin

---

**Not:** TensorRT kullanÄ±mÄ± opsiyoneldir. YÃ¼klÃ¼ deÄŸilse, proje normal PyTorch inference ile Ã§alÄ±ÅŸmaya devam eder.
