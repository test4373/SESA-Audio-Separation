#!/usr/bin/env python
# coding: utf-8
"""
TensorRT Backend KullanÄ±m Ã–rnekleri
Bu dosya TensorRT backend'inin farklÄ± kullanÄ±m senaryolarÄ±nÄ± gÃ¶sterir.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import time
import numpy as np
from tensorrt_backend import (
    TensorRTBackend, 
    is_tensorrt_available, 
    convert_model_to_tensorrt,
    load_model_from_checkpoint
)
from utils import get_model_from_config


def example_1_basic_usage():
    """
    Ã–rnek 1: Temel TensorRT Backend KullanÄ±mÄ±
    """
    print("="*60)
    print("Ã–rnek 1: Temel TensorRT Backend KullanÄ±mÄ±")
    print("="*60)
    
    # TensorRT kontrolÃ¼
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        return
    
    # Model yÃ¼kle
    model_type = 'mel_band_roformer'
    config_path = 'ckpts/config.yaml'
    checkpoint_path = 'ckpts/model.ckpt'
    
    print("ğŸ“¦ Model yÃ¼kleniyor...")
    model, config = get_model_from_config(model_type, config_path)
    model = load_model_from_checkpoint(checkpoint_path, model, 'cuda:0')
    
    # TensorRT backend oluÅŸtur
    print("ğŸ”§ TensorRT backend oluÅŸturuluyor...")
    trt_backend = TensorRTBackend(device='cuda:0')
    
    # Model compile et
    chunk_size = config.audio.chunk_size
    input_shape = (1, 2, chunk_size)  # (batch, channels, samples)
    
    print(f"âš™ï¸  Model compile ediliyor (input shape: {input_shape})...")
    trt_backend.compile_model(
        model=model,
        input_shape=input_shape,
        precision='fp16',
        use_cache=True
    )
    
    # Test inference
    print("ğŸ§ª Test inference...")
    dummy_input = torch.randn(*input_shape).cuda()
    
    with torch.no_grad():
        output = trt_backend(dummy_input)
    
    print(f"âœ“ Ã‡Ä±ktÄ± ÅŸekli: {output.shape}")
    print("âœ… Ã–rnek 1 tamamlandÄ±!\n")


def example_2_performance_comparison():
    """
    Ã–rnek 2: PyTorch vs TensorRT Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±
    """
    print("="*60)
    print("Ã–rnek 2: PyTorch vs TensorRT Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    print("="*60)
    
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        return
    
    # Model yÃ¼kle
    model_type = 'mel_band_roformer'
    config_path = 'ckpts/config.yaml'
    checkpoint_path = 'ckpts/model.ckpt'
    
    print("ğŸ“¦ Model yÃ¼kleniyor...")
    model, config = get_model_from_config(model_type, config_path)
    model = load_model_from_checkpoint(checkpoint_path, model, 'cuda:0')
    model.eval()
    
    # Test parametreleri
    chunk_size = config.audio.chunk_size
    input_shape = (1, 2, chunk_size)
    dummy_input = torch.randn(*input_shape).cuda()
    num_iterations = 50
    warmup = 10
    
    # PyTorch benchmark
    print("\nğŸ”¥ PyTorch benchmark...")
    with torch.no_grad():
        # Warmup
        for _ in range(warmup):
            _ = model(dummy_input)
        
        # Benchmark
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(num_iterations):
            _ = model(dummy_input)
        torch.cuda.synchronize()
        pytorch_time = (time.time() - start) / num_iterations
    
    print(f"  PyTorch ortalama: {pytorch_time*1000:.2f} ms")
    
    # TensorRT benchmark
    print("\nğŸš€ TensorRT benchmark...")
    trt_backend = TensorRTBackend(device='cuda:0')
    trt_backend.compile_model(
        model=model,
        input_shape=input_shape,
        precision='fp16',
        use_cache=False
    )
    
    with torch.no_grad():
        # Warmup
        for _ in range(warmup):
            _ = trt_backend(dummy_input)
        
        # Benchmark
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(num_iterations):
            _ = trt_backend(dummy_input)
        torch.cuda.synchronize()
        tensorrt_time = (time.time() - start) / num_iterations
    
    print(f"  TensorRT ortalama: {tensorrt_time*1000:.2f} ms")
    
    # KarÅŸÄ±laÅŸtÄ±rma
    speedup = pytorch_time / tensorrt_time
    print(f"\nğŸ“ˆ SonuÃ§:")
    print(f"  HÄ±zlanma: {speedup:.2f}x")
    print(f"  Ä°yileÅŸtirme: {((pytorch_time-tensorrt_time)/pytorch_time)*100:.1f}%")
    print("âœ… Ã–rnek 2 tamamlandÄ±!\n")


def example_3_save_and_load_engine():
    """
    Ã–rnek 3: TensorRT Engine Kaydetme ve YÃ¼kleme
    """
    print("="*60)
    print("Ã–rnek 3: TensorRT Engine Kaydetme ve YÃ¼kleme")
    print("="*60)
    
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        return
    
    # Model yÃ¼kle
    model_type = 'mel_band_roformer'
    config_path = 'ckpts/config.yaml'
    checkpoint_path = 'ckpts/model.ckpt'
    
    print("ğŸ“¦ Model yÃ¼kleniyor...")
    model, config = get_model_from_config(model_type, config_path)
    model = load_model_from_checkpoint(checkpoint_path, model, 'cuda:0')
    
    # TensorRT compile
    print("ğŸ”§ Model compile ediliyor...")
    trt_backend = TensorRTBackend(device='cuda:0')
    chunk_size = config.audio.chunk_size
    input_shape = (1, 2, chunk_size)
    
    trt_backend.compile_model(
        model=model,
        input_shape=input_shape,
        precision='fp16',
        use_cache=False
    )
    
    # Engine'i kaydet
    save_path = 'trt_cache/example_model.trt'
    print(f"ğŸ’¾ Engine kaydediliyor: {save_path}")
    trt_backend.save_engine(save_path)
    
    # Yeni backend oluÅŸtur ve engine'i yÃ¼kle
    print("ğŸ“¥ Engine yÃ¼kleniyor...")
    new_backend = TensorRTBackend(device='cuda:0')
    new_backend.load_engine(save_path)
    
    # Test
    print("ğŸ§ª Test inference...")
    dummy_input = torch.randn(*input_shape).cuda()
    with torch.no_grad():
        output = new_backend(dummy_input)
    
    print(f"âœ“ Ã‡Ä±ktÄ± ÅŸekli: {output.shape}")
    print("âœ… Ã–rnek 3 tamamlandÄ±!\n")


def example_4_batch_processing():
    """
    Ã–rnek 4: Batch Processing ile TensorRT
    """
    print("="*60)
    print("Ã–rnek 4: Batch Processing ile TensorRT")
    print("="*60)
    
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        return
    
    # Model yÃ¼kle
    model_type = 'mel_band_roformer'
    config_path = 'ckpts/config.yaml'
    checkpoint_path = 'ckpts/model.ckpt'
    
    print("ğŸ“¦ Model yÃ¼kleniyor...")
    model, config = get_model_from_config(model_type, config_path)
    model = load_model_from_checkpoint(checkpoint_path, model, 'cuda:0')
    
    # FarklÄ± batch size'lar iÃ§in test
    chunk_size = config.audio.chunk_size
    batch_sizes = [1, 2, 4]
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“Š Batch Size: {batch_size}")
        
        # TensorRT compile
        input_shape = (batch_size, 2, chunk_size)
        trt_backend = TensorRTBackend(device='cuda:0')
        trt_backend.compile_model(
            model=model,
            input_shape=input_shape,
            precision='fp16',
            use_cache=False
        )
        
        # Test
        dummy_input = torch.randn(*input_shape).cuda()
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = trt_backend(dummy_input)
        
        # Benchmark
        torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            for _ in range(50):
                _ = trt_backend(dummy_input)
        torch.cuda.synchronize()
        elapsed = (time.time() - start) / 50
        
        print(f"  Ortalama sÃ¼re: {elapsed*1000:.2f} ms")
        print(f"  Sample baÅŸÄ±na: {(elapsed*1000)/batch_size:.2f} ms")
    
    print("\nâœ… Ã–rnek 4 tamamlandÄ±!\n")


def example_5_precision_comparison():
    """
    Ã–rnek 5: FP32 vs FP16 Precision KarÅŸÄ±laÅŸtÄ±rmasÄ±
    """
    print("="*60)
    print("Ã–rnek 5: FP32 vs FP16 Precision KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    print("="*60)
    
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        return
    
    # Model yÃ¼kle
    model_type = 'mel_band_roformer'
    config_path = 'ckpts/config.yaml'
    checkpoint_path = 'ckpts/model.ckpt'
    
    print("ğŸ“¦ Model yÃ¼kleniyor...")
    model, config = get_model_from_config(model_type, config_path)
    model = load_model_from_checkpoint(checkpoint_path, model, 'cuda:0')
    
    chunk_size = config.audio.chunk_size
    input_shape = (1, 2, chunk_size)
    dummy_input = torch.randn(*input_shape).cuda()
    
    results = {}
    
    for precision in ['fp32', 'fp16']:
        print(f"\nğŸ”§ {precision.upper()} precision ile compile ediliyor...")
        
        trt_backend = TensorRTBackend(device='cuda:0')
        trt_backend.compile_model(
            model=model,
            input_shape=input_shape,
            precision=precision,
            use_cache=False
        )
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = trt_backend(dummy_input)
        
        # Benchmark
        torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            for _ in range(50):
                output = trt_backend(dummy_input)
        torch.cuda.synchronize()
        elapsed = (time.time() - start) / 50
        
        results[precision] = {
            'time': elapsed * 1000,
            'output': output
        }
        
        print(f"  Ortalama sÃ¼re: {elapsed*1000:.2f} ms")
    
    # DoÄŸruluk karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("\nğŸ” DoÄŸruluk karÅŸÄ±laÅŸtÄ±rmasÄ±:")
    fp32_out = results['fp32']['output']
    fp16_out = results['fp16']['output']
    
    diff = torch.abs(fp32_out - fp16_out)
    print(f"  Max fark: {torch.max(diff).item():.6f}")
    print(f"  Mean fark: {torch.mean(diff).item():.6f}")
    
    # HÄ±z karÅŸÄ±laÅŸtÄ±rmasÄ±
    speedup = results['fp32']['time'] / results['fp16']['time']
    print(f"\nğŸ“ˆ FP16 hÄ±zlanma: {speedup:.2f}x")
    
    print("\nâœ… Ã–rnek 5 tamamlandÄ±!\n")


def main():
    """
    TÃ¼m Ã¶rnekleri Ã§alÄ±ÅŸtÄ±r
    """
    print("\n" + "="*60)
    print("TensorRT Backend KullanÄ±m Ã–rnekleri")
    print("="*60 + "\n")
    
    if not is_tensorrt_available():
        print("âŒ TensorRT mevcut deÄŸil!")
        print("Kurulum iÃ§in: pip install torch2trt nvidia-tensorrt")
        return
    
    print("GPU:", torch.cuda.get_device_name(0))
    print("CUDA:", torch.version.cuda)
    print()
    
    try:
        # Ã–rnek 1: Temel kullanÄ±m
        example_1_basic_usage()
        
        # Ã–rnek 2: Performans karÅŸÄ±laÅŸtÄ±rmasÄ±
        example_2_performance_comparison()
        
        # Ã–rnek 3: Save/Load
        example_3_save_and_load_engine()
        
        # Ã–rnek 4: Batch processing
        example_4_batch_processing()
        
        # Ã–rnek 5: Precision karÅŸÄ±laÅŸtÄ±rmasÄ±
        example_5_precision_comparison()
        
        print("\n" + "="*60)
        print("ğŸ‰ TÃ¼m Ã¶rnekler baÅŸarÄ±yla tamamlandÄ±!")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
